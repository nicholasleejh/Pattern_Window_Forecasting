{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pandas_ta as ta\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Define the tickers and the date range\n",
    "tickers = ['SPY', 'IWM', 'DIA']\n",
    "start_date = '2012-11-01'\n",
    "end_date = '2024-12-31'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Manual Calculation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_cmfi(input_df, window):\n",
    "    \"\"\"Chaikin Money Flow for a specific window\"\"\"\n",
    "    mf_multiplier = ((input_df['Close'] - input_df['Low']) - (input_df['High'] - input_df['Close'])) / (input_df['High'] - input_df['Low'])\n",
    "    mf_volume = mf_multiplier * input_df['Volume']\n",
    "    return mf_volume.rolling(window=window).sum() / input_df['Volume'].rolling(window=window).sum()\n",
    "\n",
    "def calculate_dmi(input_df, window):\n",
    "    \"\"\"Directional Movement Index (ADX) with proper Pandas Series conversion\"\"\"\n",
    "    # Calculate True Range\n",
    "    tr = pd.Series(np.maximum(\n",
    "        input_df['High'] - input_df['Low'],\n",
    "        np.maximum(\n",
    "            abs(input_df['High'] - input_df['Close'].shift(1)),\n",
    "            abs(input_df['Low'] - input_df['Close'].shift(1))\n",
    "        )\n",
    "    ), index=input_df.index)\n",
    "\n",
    "    # Calculate Directional Movement\n",
    "    up = input_df['High'].diff()\n",
    "    down = -input_df['Low'].diff()\n",
    "    \n",
    "    # Convert to Pandas Series immediately after np.where\n",
    "    plus_dm = pd.Series(np.where((up > down) & (up > 0), up, 0), index=input_df.index)\n",
    "    minus_dm = pd.Series(np.where((down > up) & (down > 0), down, 0), index=input_df.index)\n",
    "    \n",
    "    # Smoothing (now works because we're using Pandas Series)\n",
    "    plus_di = 100 * (plus_dm.ewm(alpha=1/window, adjust=False).mean() / \n",
    "                    tr.ewm(alpha=1/window, adjust=False).mean())\n",
    "    minus_di = 100 * (minus_dm.ewm(alpha=1/window, adjust=False).mean() / \n",
    "                     tr.ewm(alpha=1/window, adjust=False).mean())\n",
    "    \n",
    "    dx = 100 * abs(plus_di - minus_di) / (plus_di + minus_di)\n",
    "    return dx.ewm(alpha=1/window, adjust=False).mean()  # ADX\n",
    "\n",
    "def calculate_psar_window(high, low, initial_af=0.02, max_af=0.2, step=0.02):\n",
    "    \"\"\"Calculate Parabolic SAR for a range of windows by adjusting AF step size\"\"\"\n",
    "    psar = [low.iloc[0]]\n",
    "    ep = high.iloc[0]\n",
    "    af = initial_af\n",
    "    uptrend = True\n",
    "    \n",
    "    for i in range(1, len(high)):\n",
    "        prev_psar = psar[-1]\n",
    "        \n",
    "        if uptrend:\n",
    "            current_psar = prev_psar + af * (ep - prev_psar)\n",
    "            if low.iloc[i] < current_psar:\n",
    "                uptrend = False\n",
    "                current_psar = ep\n",
    "                ep = low.iloc[i]\n",
    "                af = initial_af\n",
    "            else:\n",
    "                if high.iloc[i] > ep:\n",
    "                    ep = high.iloc[i]\n",
    "                    af = min(af + step, max_af)\n",
    "        else:\n",
    "            current_psar = prev_psar - af * (prev_psar - ep)\n",
    "            if high.iloc[i] > current_psar:\n",
    "                uptrend = True\n",
    "                current_psar = ep\n",
    "                ep = high.iloc[i]\n",
    "                af = initial_af\n",
    "            else:\n",
    "                if low.iloc[i] < ep:\n",
    "                    ep = low.iloc[i]\n",
    "                    af = min(af + step, max_af)\n",
    "        psar.append(current_psar)\n",
    "    \n",
    "    return pd.Series(psar, index=high.index)\n",
    "\n",
    "def calculate_all_psar(df, windows=range(6, 21)):\n",
    "    \"\"\"Calculate PSAR for multiple window sizes by adjusting AF step\"\"\"\n",
    "    for n in windows:\n",
    "        # Scale AF step inversely with window size (shorter windows = faster AF increase)\n",
    "        step = 0.02 * (14 / n)  # Normalized to 14-day baseline\n",
    "        df[f'PSAR_{n}'] = calculate_psar_window(df['High'], df['Low'], step=step)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_labelling(df, upper_mult=2.0, lower_mult=1.5, max_days=15):\n",
    "    \"\"\"Volatility-adjusted labeling using pandas_ta with first-hit priority\"\"\"\n",
    "    close = df['Close'].values\n",
    "    \n",
    "    # Calculate ATR using pandas_ta\n",
    "    df['ATR'] = ta.atr(df['High'], df['Low'], df['Close'], length=max_days)\n",
    "    atr = df['ATR'].values\n",
    "    \n",
    "    labels = np.zeros(len(close))\n",
    "    for i in range(len(close) - max_days):\n",
    "        upper = close[i] + upper_mult * atr[i]\n",
    "        lower = close[i] - lower_mult * atr[i]\n",
    "        future_window = close[i+1:i+max_days+1]\n",
    "        \n",
    "        # Determine if upper or lower barrier is hit\n",
    "        upper_hit_indices = np.where(future_window >= upper)[0]  # Find indices where upper is hit\n",
    "        lower_hit_indices = np.where(future_window <= lower)[0]  # Find indices where lower is hit\n",
    "        \n",
    "        if upper_hit_indices.size > 0 and lower_hit_indices.size > 0:\n",
    "            # If both upper and lower barriers are hit, assign label based on the first barrier hit\n",
    "            if upper_hit_indices[0] < lower_hit_indices[0]:\n",
    "                labels[i] = 2  # Upper barrier hit first\n",
    "            else:\n",
    "                labels[i] = 0  # Lower barrier hit first\n",
    "        elif upper_hit_indices.size > 0:\n",
    "            labels[i] = 2  # Upper barrier hit\n",
    "        elif lower_hit_indices.size > 0:\n",
    "            labels[i] = 0  # Lower barrier hit\n",
    "        else:\n",
    "            labels[i] = 1  # Time barrier\n",
    "    \n",
    "    df['Label'] = labels\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Data, Indicators & Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  3 of 3 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing SPY...\n",
      "Saved SPY_2019_2020.csv\n",
      "Saved SPY_2020_2021.csv\n",
      "Saved SPY_2021_2022.csv\n",
      "Saved SPY_2022_2023.csv\n",
      "Saved SPY_2023_2024.csv\n",
      "\n",
      "Processing IWM...\n",
      "Saved IWM_2019_2020.csv\n",
      "Saved IWM_2020_2021.csv\n",
      "Saved IWM_2021_2022.csv\n",
      "Saved IWM_2022_2023.csv\n",
      "Saved IWM_2023_2024.csv\n",
      "\n",
      "Processing DIA...\n",
      "Saved DIA_2019_2020.csv\n",
      "Saved DIA_2020_2021.csv\n",
      "Saved DIA_2021_2022.csv\n",
      "Saved DIA_2022_2023.csv\n",
      "Saved DIA_2023_2024.csv\n"
     ]
    }
   ],
   "source": [
    "data = yf.download(tickers, start=start_date, end=end_date, interval='1d', group_by='ticker')\n",
    "\n",
    "for ticker in tickers:\n",
    "    print(f\"\\nProcessing {ticker}...\")\n",
    "    df = data[ticker].dropna().copy()\n",
    "    df = df.reset_index()\n",
    "    df['Date'] = pd.to_datetime(df['Date'])\n",
    "    \n",
    "    # Pre-allocate all indicator data\n",
    "    indicator_data = {}\n",
    "    \n",
    "    for n in range(6, 21):\n",
    "        # Store indicators in dictionary first\n",
    "        indicator_data[f'SMA_{n}'] = ta.sma(df['Close'], length=n)\n",
    "        indicator_data[f'EMA_{n}'] = ta.ema(df['Close'], length=n)\n",
    "        indicator_data[f'WMA_{n}'] = ta.wma(df['Close'], length=n)\n",
    "        indicator_data[f'HMA_{n}'] = ta.hma(df['Close'], length=n)\n",
    "        indicator_data[f'TEMA_{n}'] = ta.tema(df['Close'], length=n)\n",
    "        # Momentum indicators\n",
    "        indicator_data[f'RSI_{n}'] = ta.rsi(df['Close'], length=n)\n",
    "        indicator_data[f'Williams_%R_{n}'] = ta.willr(df['High'], df['Low'], df['Close'], length=n)\n",
    "        indicator_data[f'CMO_{n}'] = ta.cmo(df['Close'], length=n)\n",
    "        indicator_data[f'ROC_{n}'] = ta.roc(df['Close'], length=n)\n",
    "        # Composite indicators\n",
    "        macd = ta.macd(df['Close'], fast=n, slow=min(2*n, 26), signal=9)\n",
    "        indicator_data[f'MACD_{n}'] = macd[f'MACD_{n}_{min(2*n,26)}_9']\n",
    "        ppo = ta.ppo(df['Close'], fast=n, slow=min(2*n, 26), signal=9)\n",
    "        indicator_data[f'PPO_{n}'] = ppo[f'PPO_{n}_{min(2*n,26)}_9']\n",
    "        indicator_data[f'CCI_{n}'] = ta.cci(df['High'], df['Low'], df['Close'], length=n)\n",
    "        # Volume/trend\n",
    "        indicator_data[f'CMFI_{n}'] = calculate_cmfi(df, n)\n",
    "        indicator_data[f'DMI_{n}'] = calculate_dmi(df, n)\n",
    "    \n",
    "    # Add PSAR (windowed)\n",
    "    psar_data = {f'PSAR_{n}': calculate_psar_window(df['High'], df['Low'], step=0.02*(14/n)) \n",
    "                for n in range(6, 21)}\n",
    "    indicator_data.update(psar_data)\n",
    "\n",
    "    # Combine all at once\n",
    "    full_df = pd.concat([df, pd.DataFrame(indicator_data, index=df.index)], axis=1)\n",
    "\n",
    "    # Apply labelling\n",
    "    full_df = apply_labelling(full_df)\n",
    "\n",
    "    # Drop unused columns\n",
    "    full_df = full_df.drop(columns=['Open', 'High', 'Low', 'Volume'])\n",
    "    # full_df = full_df.drop(columns=['Open', 'High', 'Low', 'Volume', 'Returns', 'SMA', 'Upper_BB', 'Lower_BB', 'RSI', 'ATR'])\n",
    "\n",
    "    # Create 8-year rolling windows\n",
    "    for year in range(2013, 2018):  # 2013-2020, 2014-2021, ..., 2017-2024\n",
    "        window_start = f\"{year}-01-01\"\n",
    "        window_end = f\"{year+7}-12-31\"\n",
    "        window_df = full_df[(full_df['Date'] >= window_start) & (full_df['Date'] <= window_end)]\n",
    "        \n",
    "        if len(window_df) > 0:\n",
    "            window_df.to_csv(f'./data/test_years/{ticker}_{year+6}_{year+7}.csv', index=False)\n",
    "            print(f\"Saved {ticker}_{year+6}_{year+7}.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved SPY_2019_2020.csv\n",
      "Saved SPY_2020_2021.csv\n",
      "Saved SPY_2021_2022.csv\n",
      "Saved SPY_2022_2023.csv\n",
      "Saved SPY_2023_2024.csv\n",
      "Saved IWM_2019_2020.csv\n",
      "Saved IWM_2020_2021.csv\n",
      "Saved IWM_2021_2022.csv\n",
      "Saved IWM_2022_2023.csv\n",
      "Saved IWM_2023_2024.csv\n",
      "Saved DIA_2019_2020.csv\n",
      "Saved DIA_2020_2021.csv\n",
      "Saved DIA_2021_2022.csv\n",
      "Saved DIA_2022_2023.csv\n",
      "Saved DIA_2023_2024.csv\n",
      "Normalised data saved.\n"
     ]
    }
   ],
   "source": [
    "for ticker in tickers:\n",
    "    for year in range(2019, 2024):\n",
    "        df = pd.read_csv(f'./data/test_years/{ticker}_{year}_{year+1}.csv')\n",
    "        df['Date'] = pd.to_datetime(df['Date'])\n",
    "\n",
    "        # Split into periods\n",
    "        train = df[df['Date'].dt.year <= year-2]\n",
    "        val = df[df['Date'].dt.year == year-1]\n",
    "        test = df[df['Date'].dt.year >= year]\n",
    "\n",
    "        # Identify columns to normalize (all except Date/Close/Label)\n",
    "        exclude_cols = ['Date', 'Close', 'Label']\n",
    "        feature_cols = [col for col in df.columns if col not in exclude_cols]\n",
    "        \n",
    "        # Normalize using train stats\n",
    "        scaler = StandardScaler()\n",
    "        train.loc[:, feature_cols] = scaler.fit_transform(train[feature_cols])\n",
    "        val.loc[:, feature_cols] = scaler.transform(val[feature_cols])\n",
    "        test.loc[:, feature_cols] = scaler.transform(test[feature_cols])\n",
    "\n",
    "        # Combine back and save\n",
    "        normalised_df = pd.concat([train, val, test], axis=0)\n",
    "        normalised_df.to_csv(f'./data/normalised/{ticker}_{year}_{year+1}.csv', index=False)\n",
    "\n",
    "        print(f\"Saved {ticker}_{year}_{year+1}.csv\")\n",
    "\n",
    "print('Normalised data saved.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
