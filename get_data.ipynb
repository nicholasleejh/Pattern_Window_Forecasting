{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pandas_ta as ta\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Define the tickers and the date range\n",
    "tickers = ['XLF', 'XLU', 'QQQ', 'SPY', 'XLP', 'EWZ', 'EWH', 'XLY', 'XLE']\n",
    "start_date = '2013-11-01'\n",
    "end_date = '2024-12-31'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Manual Calculation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Chaikin Money Flow Indicator (CMFI)\n",
    "def calculate_cmfi(input_df):\n",
    "    ticker_data = input_df.copy()\n",
    "    mf_multiplier = ((ticker_data['Close'] - ticker_data['Low']) - (ticker_data['High'] - ticker_data['Close'])) / (ticker_data['High'] - ticker_data['Low'])\n",
    "    mf_volume = mf_multiplier * ticker_data['Volume']\n",
    "    return mf_volume.rolling(window=20).sum() / ticker_data['Volume'].rolling(window=20).sum()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Calculate Directional Movement Indicator (DMI)\n",
    "def calculate_dmi(input_df):\n",
    "    ticker_data = input_df.copy()\n",
    "    ticker_data['TR'] = np.maximum(\n",
    "        ticker_data['High'] - ticker_data['Low'],\n",
    "        np.maximum(\n",
    "            abs(ticker_data['High'] - ticker_data['Close'].shift(1)),\n",
    "            abs(ticker_data['Low'] - ticker_data['Close'].shift(1))\n",
    "        )\n",
    "    )\n",
    "    ticker_data['UpMove'] = ticker_data['High'].diff()\n",
    "    ticker_data['DownMove'] = ticker_data['Low'].shift(1) - ticker_data['Low']\n",
    "\n",
    "    ticker_data['+DMI'] = np.where((ticker_data['UpMove'] > ticker_data['DownMove']) & (ticker_data['UpMove'] > 0), ticker_data['UpMove'], 0)\n",
    "    ticker_data['-DMI'] = np.where((ticker_data['DownMove'] > ticker_data['UpMove']) & (ticker_data['DownMove'] > 0), ticker_data['DownMove'], 0)\n",
    "\n",
    "    ticker_data['Smoothed +DMI'] = ticker_data['+DMI'].ewm(span=14, adjust=False).mean()\n",
    "    ticker_data['Smoothed -DMI'] = ticker_data['-DMI'].ewm(span=14, adjust=False).mean()\n",
    "    ticker_data['Smoothed TR'] = ticker_data['TR'].ewm(span=14, adjust=False).mean()\n",
    "\n",
    "    ticker_data['+DI'] = 100 * (ticker_data['Smoothed +DMI'] / ticker_data['Smoothed TR'])\n",
    "    ticker_data['-DI'] = 100 * (ticker_data['Smoothed -DMI'] / ticker_data['Smoothed TR'])\n",
    "\n",
    "    ticker_data['DX'] = 100 * (abs(ticker_data['+DI'] - ticker_data['-DI']) / (ticker_data['+DI'] + ticker_data['-DI']))\n",
    "    return ticker_data['DX'].ewm(span=14, adjust=False).mean()  # Use ADX as the DMI column\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Calculate Parabolic SAR (PSAR)\n",
    "def calculate_psar(high, low):\n",
    "    # Initialize variables\n",
    "    psar = [low.iloc[0]]  # Start with the first low as the initial SAR\n",
    "    ep = high.iloc[0]  # Extreme Point (EP) starts as the first high\n",
    "    af = 0.02  # Acceleration Factor (AF) starts at 0.02\n",
    "    uptrend = True  # Assume the trend starts as rising\n",
    "\n",
    "    for i in range(1, len(high)):\n",
    "        prev_psar = psar[-1]\n",
    "\n",
    "        if uptrend:\n",
    "            # Rising SAR\n",
    "            current_psar = prev_psar + af * (ep - prev_psar)\n",
    "            if low.iloc[i] < current_psar:  # Trend reversal\n",
    "                uptrend = False\n",
    "                current_psar = ep  # Reset SAR to the EP\n",
    "                ep = low.iloc[i]  # Reset EP to the current low\n",
    "                af = 0.02  # Reset AF\n",
    "            else:\n",
    "                if high.iloc[i] > ep:  # Update EP and AF\n",
    "                    ep = high.iloc[i]\n",
    "                    af = min(af + 0.02, 0.2)  # Cap AF at 0.2\n",
    "        else:\n",
    "            # Falling SAR\n",
    "            current_psar = prev_psar - af * (prev_psar - ep)\n",
    "            if high.iloc[i] > current_psar:  # Trend reversal\n",
    "                uptrend = True\n",
    "                current_psar = ep  # Reset SAR to the EP\n",
    "                ep = high.iloc[i]  # Reset EP to the current high\n",
    "                af = 0.02  # Reset AF\n",
    "            else:\n",
    "                if low.iloc[i] < ep:  # Update EP and AF\n",
    "                    ep = low.iloc[i]\n",
    "                    af = min(af + 0.02, 0.2)  # Cap AF at 0.2\n",
    "\n",
    "        psar.append(current_psar)\n",
    "\n",
    "    return psar\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def apply_labelling(data, window_size=11):\n",
    "    \"\"\"\n",
    "    Apply the labelling algorithm to the data.\n",
    "    Labels:\n",
    "        - BUY: If the minimum price in the window is at the middle index.\n",
    "        - SELL: If the maximum price in the window is at the middle index.\n",
    "        - HOLD: Otherwise.\n",
    "    \"\"\"\n",
    "    labels = []\n",
    "    close_prices = data['Close'].values\n",
    "\n",
    "    for counter_row in range(len(close_prices)):\n",
    "        if counter_row + 1 >= window_size:\n",
    "            # Define the window\n",
    "            window_begin_index = counter_row + 1 - window_size\n",
    "            window_end_index = counter_row\n",
    "            window_middle_index = (window_begin_index + window_end_index) // 2\n",
    "\n",
    "            # Extract the window of close prices\n",
    "            window = close_prices[window_begin_index:window_end_index + 1]\n",
    "\n",
    "            # Find min and max in the window\n",
    "            min_price = window.min()\n",
    "            max_price = window.max()\n",
    "            min_index = window_begin_index + window.tolist().index(min_price)\n",
    "            max_index = window_begin_index + window.tolist().index(max_price)\n",
    "\n",
    "            # Assign labels\n",
    "            if max_index == window_middle_index:\n",
    "                labels.append('SELL')\n",
    "            elif min_index == window_middle_index:\n",
    "                labels.append('BUY')\n",
    "            else:\n",
    "                labels.append('HOLD')\n",
    "        else:\n",
    "            # Not enough data for the window\n",
    "            labels.append('HOLD')\n",
    "\n",
    "    # Add labels to the DataFrame\n",
    "    data['Label'] = labels\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Data, Indicators & Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  9 of 9 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing XLF...\n",
      "Saved XLF_indicators.csv\n",
      "Processing XLU...\n",
      "Saved XLU_indicators.csv\n",
      "Processing QQQ...\n",
      "Saved QQQ_indicators.csv\n",
      "Processing SPY...\n",
      "Saved SPY_indicators.csv\n",
      "Processing XLP...\n",
      "Saved XLP_indicators.csv\n",
      "Processing EWZ...\n",
      "Saved EWZ_indicators.csv\n",
      "Processing EWH...\n",
      "Saved EWH_indicators.csv\n",
      "Processing XLY...\n",
      "Saved XLY_indicators.csv\n",
      "Processing XLE...\n",
      "Saved XLE_indicators.csv\n",
      "Processing complete.\n"
     ]
    }
   ],
   "source": [
    "# Download the data\n",
    "data = yf.download(tickers, start=start_date, end=end_date, interval='1d', group_by='ticker')\n",
    "\n",
    "# Loop through each ticker and calculate indicators\n",
    "for ticker in tickers:\n",
    "    print(f\"Processing {ticker}...\")\n",
    "    ticker_data = data[ticker].dropna()  # Get data for the specific ticker and drop NaN values\n",
    "\n",
    "    # Calculate indicators\n",
    "    ticker_data['RSI'] = ta.rsi(ticker_data['Close'])\n",
    "    ticker_data['Williams %R'] = ta.willr(ticker_data['High'], ticker_data['Low'], ticker_data['Close'])\n",
    "    ticker_data['SMA'] = ta.sma(ticker_data['Close'])\n",
    "    ticker_data['EMA'] = ta.ema(ticker_data['Close'])\n",
    "    ticker_data['WMA'] = ta.wma(ticker_data['Close'])\n",
    "    ticker_data['HMA'] = ta.hma(ticker_data['Close'])\n",
    "    ticker_data['TEMA'] = ta.tema(ticker_data['Close'])\n",
    "    ticker_data['CCI'] = ta.cci(ticker_data['High'], ticker_data['Low'], ticker_data['Close'])\n",
    "    ticker_data['CMO'] = ta.cmo(ticker_data['Close'])\n",
    "    ticker_data['MACD'] = ta.macd(ticker_data['Close'])['MACD_12_26_9']\n",
    "    ticker_data['PPO']= ta.ppo(ticker_data['Close'])['PPO_12_26_9']\n",
    "    ticker_data['ROC'] = ta.roc(ticker_data['Close'])\n",
    "    ticker_data['CMFI'] = calculate_cmfi(ticker_data)\n",
    "    ticker_data['DMI'] = calculate_dmi(ticker_data)\n",
    "    ticker_data['Parabolic SAR'] = calculate_psar(ticker_data['High'], ticker_data['Low'])\n",
    "\n",
    "    # Apply labelling algorithm\n",
    "    ticker_data = apply_labelling(ticker_data)\n",
    "\n",
    "    # Drop unused columns\n",
    "    ticker_data = ticker_data.drop(columns=['Open', 'High', 'Low', 'Close', 'Volume'])\n",
    "\n",
    "    # Extract data for 2014-2024\n",
    "    ticker_data.loc['2014-01-01':'2024-12-31'].to_csv(f'./data/indicators/{ticker}_indicators.csv')\n",
    "\n",
    "    # Checkpoint\n",
    "    print(f\"Saved {ticker}_indicators.csv\")\n",
    "\n",
    "# Done!\n",
    "print(\"Processing complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate 15-Day Windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate 15-day sliding windows for each column\n",
    "def generate_15_day_windows(data, label_column, window_size=15):\n",
    "    # Ensure the label column is not included in the sliding window\n",
    "    feature_columns = [col for col in data.columns if col not in [label_column, 'Date']]\n",
    "    \n",
    "    # Create a new DataFrame to store the results\n",
    "    result = []\n",
    "\n",
    "    for i in range(window_size, len(data)):\n",
    "        # Extract the 15-day window for each column\n",
    "        row = {col: data[col].iloc[i - window_size:i].tolist() for col in feature_columns}\n",
    "        # Add the label for the current day\n",
    "        row[label_column] = data[label_column].iloc[i]\n",
    "        # Add the date for the current day\n",
    "        row['Date'] = data['Date'].iloc[i]\n",
    "        result.append(row)\n",
    "\n",
    "    # Convert the result to a DataFrame\n",
    "    return pd.DataFrame(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing XLF...\n",
      "Saved data for window 2014-2019.\n",
      "Saved data for window 2015-2020.\n",
      "Saved data for window 2016-2021.\n",
      "Saved data for window 2017-2022.\n",
      "Saved data for window 2018-2023.\n",
      "Saved data for window 2019-2024.\n",
      "Processing XLU...\n",
      "Saved data for window 2014-2019.\n",
      "Saved data for window 2015-2020.\n",
      "Saved data for window 2016-2021.\n",
      "Saved data for window 2017-2022.\n",
      "Saved data for window 2018-2023.\n",
      "Saved data for window 2019-2024.\n",
      "Processing QQQ...\n",
      "Saved data for window 2014-2019.\n",
      "Saved data for window 2015-2020.\n",
      "Saved data for window 2016-2021.\n",
      "Saved data for window 2017-2022.\n",
      "Saved data for window 2018-2023.\n",
      "Saved data for window 2019-2024.\n",
      "Processing SPY...\n",
      "Saved data for window 2014-2019.\n",
      "Saved data for window 2015-2020.\n",
      "Saved data for window 2016-2021.\n",
      "Saved data for window 2017-2022.\n",
      "Saved data for window 2018-2023.\n",
      "Saved data for window 2019-2024.\n",
      "Processing XLP...\n",
      "Saved data for window 2014-2019.\n",
      "Saved data for window 2015-2020.\n",
      "Saved data for window 2016-2021.\n",
      "Saved data for window 2017-2022.\n",
      "Saved data for window 2018-2023.\n",
      "Saved data for window 2019-2024.\n",
      "Processing EWZ...\n",
      "Saved data for window 2014-2019.\n",
      "Saved data for window 2015-2020.\n",
      "Saved data for window 2016-2021.\n",
      "Saved data for window 2017-2022.\n",
      "Saved data for window 2018-2023.\n",
      "Saved data for window 2019-2024.\n",
      "Processing EWH...\n",
      "Saved data for window 2014-2019.\n",
      "Saved data for window 2015-2020.\n",
      "Saved data for window 2016-2021.\n",
      "Saved data for window 2017-2022.\n",
      "Saved data for window 2018-2023.\n",
      "Saved data for window 2019-2024.\n",
      "Processing XLY...\n",
      "Saved data for window 2014-2019.\n",
      "Saved data for window 2015-2020.\n",
      "Saved data for window 2016-2021.\n",
      "Saved data for window 2017-2022.\n",
      "Saved data for window 2018-2023.\n",
      "Saved data for window 2019-2024.\n",
      "Processing XLE...\n",
      "Saved data for window 2014-2019.\n",
      "Saved data for window 2015-2020.\n",
      "Saved data for window 2016-2021.\n",
      "Saved data for window 2017-2022.\n",
      "Saved data for window 2018-2023.\n",
      "Saved data for window 2019-2024.\n"
     ]
    }
   ],
   "source": [
    "# Define the sliding window parameters\n",
    "start_year = 2014\n",
    "end_year = 2024\n",
    "window_size = 6  # 6 years per window\n",
    "\n",
    "# Loop through each ticker and calculate indicators\n",
    "for ticker in tickers:\n",
    "    print(f\"Processing {ticker}...\")\n",
    "    \n",
    "    data = pd.read_csv(f'./data/indicators/{ticker}_indicators.csv', parse_dates=['Date'])\n",
    "\n",
    "    # Iterate through the sliding windows\n",
    "    for start in range(start_year, end_year - window_size + 2):\n",
    "        # Define the start and end years for the current window\n",
    "        window_start = start\n",
    "        window_end = start + window_size - 1\n",
    "        \n",
    "        # Filter the data for the current window\n",
    "        window_data = data[(data['Date'].dt.year >= window_start) & (data['Date'].dt.year <= window_end)]\n",
    "\n",
    "        # Split into training (first 5 years) and test (final year) sets\n",
    "        train_data = window_data[window_data['Date'].dt.year < window_end]\n",
    "        test_data = window_data[window_data['Date'].dt.year == window_end]\n",
    "\n",
    "        # Extract feature columns (exclude Date and Label columns)\n",
    "        feature_columns = [col for col in window_data.columns if col not in ['Date', 'Label']]\n",
    "\n",
    "        # Normalize the training set\n",
    "        scaler = StandardScaler()\n",
    "        train_data.loc[:, feature_columns] = scaler.fit_transform(train_data[feature_columns])\n",
    "\n",
    "        # Apply the same transformation to the test set\n",
    "        test_data.loc[:, feature_columns] = scaler.transform(test_data[feature_columns])\n",
    "        \n",
    "        # Combine the normalized training and test sets\n",
    "        combined_data = pd.concat([train_data, test_data])\n",
    "\n",
    "        # Generate 15 day lagged windows\n",
    "        lagged_data = generate_15_day_windows(combined_data, \"Label\")\n",
    "\n",
    "        # Save the filtered data to a CSV file\n",
    "        lagged_data.to_csv(f'./data/test_years/{ticker}_{window_end}.csv', index=False)\n",
    "        \n",
    "        print(f\"Saved data for window {window_start}-{window_end}.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
